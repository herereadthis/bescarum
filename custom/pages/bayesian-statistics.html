<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>A/B Testing</title>
    <meta name="author" content="James Ha">

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="../../../css/reveal.min.css">
    <link rel="stylesheet" href="../css/foo.min.css">
    <link rel="stylesheet" href="../../../css/theme/custom.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="../../../lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>
</head>
<body>
<div class="reveal">
    <div class="slides">
        <section
                data-transition="fade-in">
            <h1>3. Bayesian Statistics</h1>
            <ul>
                <li>Conclusions are subjective and can be updated</li>
                <li>Use prior knowledge and iterate</li>
                <li>Machine learning, spam filters, weather forcasting</li>
                <li>Uncertainty can be modeled</li>
                <li>Easier to reason, at least at a high level.</li>
            </ul>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
        </section>
        <section
                data-transition="fade-in">
            <h2>Bayesian Statistics: Weaknesses</h2>
            <ul>
                <li>What exactly a "Prior?"</li>
                <li>What if you have no prior?</li>
                <li>It's not wrong to want a simple a yes/no answer.</li>
                <li>Bayesian math is crazy nuts complicated</li>
                <li>Simulations are computationally intense</li>
            </ul>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
            <aside class="notes" data-markdown>
                * Perhaps that was why it was mostly ignored during much of the 20th century. Computers just weren't
                good enough.
            </aside>
        </section>
        <section

                data-background-image="../media/explosions.jpg"
                data-transition="fade-in">
            <h2 class="shadow-text" style="text-transform: uppercase;">Michael Bay</h2><br/>
            <img src="../media/michael-bay.jpg" style="height: 45vh"/><br/>
            <p class="shadow-text"><em>Statistics just got real.</em></p>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
            <aside class="notes" data-markdown>
                * R.A. Fisher is the father of Frequentist Statistics, Michael Bay is the father of Bayesian statistics
                * Michael Bay published "An Essay towards solving a a problem in the doctrine of chances," which created
                the foundations for conditional probability.
            </aside>
        </section>
        <section
                data-transition="fade-in">
            <h2>Michael Bay&rsquo;s Theorem</h2>
            <div>Muliplication rule: \(P(A \cap B) = P(A|B) \cdot P(B)\)</div>
            <div class="math-formula fragment">$$P(B|A) = \frac{P(A|B)P(B)}{P(A)}$$</div>
            <div class="fragment">
                <div class="math-formula">$$P(B|A)P(A) = P(A \cap B) = P(A|B)P(B)$$</div>
                <div class="math-formula">$$P(B|A)P(A) = P(A|B)P(B)$$</div>
            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
        </section>
        <section
                data-transition="fade-in">
            <h2>Bayes&rsquo; Theorem Again!</h2>
            <div class="math-formula">$$P(\mathcal{H}|\mathcal{D}) =
                \frac{P(\mathcal{D}|\mathcal{H})P(\mathcal{H})}{P(\mathcal{D})}$$
            </div>
            <div class="">$$P(Hypothesis|Data) = \frac{P(Data|Hypothesis)P(Hypothesis)}{P(Data)}$$</div>
            <div class="math-formula fragment">$$Posterior = \frac{Likelihood \times Prior}{Evidence}$$</div>
        </section>
        <section
                data-transition="fade-in">
            <h2>Wingardium Leviosa</h2>
            <p><em>1 out of every 10,000 people have magical midichlorians. Hogwarts can identify magical children with
                99.5% accuracy. An owl just gave you an acceptance letter.</em><br/>
                <span class="fragment"><em><strong>Question: are you magical?</strong></em></span></p>
            <ul class="fragment">
                <li>Likelihood: <span style="font-size: 0.85em;">\(P(\mathcal{D}|\mathcal{H}) = 0.995\)</span> | Prior:
                    <span style="font-size: 0.85em;">\(P(\mathcal{H}) = 0.0001\)</span></li>
                <li>Evidence: <span style="font-size: 0.85em;">\({P(\mathcal{D}) = 0.995 \cdot 0.0001 + 0.005 \cdot 0.9999 = 0.005099}\)</span>
                </li>
            </ul>
            <div class="math-formula fragment">$$P(\mathcal{H}|\mathcal{D}) = \frac{0.995\times0.0001}{0.005099} =
                1.95\%$$
            </div>

            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
            <aside class="notes">
                <ul>
                    <li>In other words, the probability of having magical abilities is 0.0001</li>
                    <li>What is the probability that a person is magical, given that they test positive for magic
                        ability?
                    </li>
                    <li>Hypothesis: H = person has magical potential</li>
                    <li>Data: D = the Hogwarts test is positive</li>
                    <li>Prior - probability of a positive test if you are magical</li>
                    <li>Likelihood - probability of having magical midichlorians</li>
                    <li>Evidence - a true positive test for those who are magical, and a false positive for those who
                        are just plain muggles
                    </li>
                </ul>
            </aside>
        </section>
        <section
                data-transition="fade-in">
            <div>
                <h2>Update Table</h2>

                <table>
                    <thead style="font-size: 85%">
                    <tr>
                        <th>Hypothesis</th>
                        <th>Prior</th>
                        <th>Likelihood</th>
                        <th>Numerator</th>
                        <th>Posterior</th>
                    </tr>
                    </thead>
                    <tbody style="font-size: 85%">
                    <tr>
                        <td>\(\theta\)</td>
                        <td>\(p(\theta)\)</td>
                        <td>\(p(x=1|\theta)\)</td>
                        <td>\(p(\theta)p(x=1|\theta)\)</td>
                        <td>\(p(\theta|x=1)\)</td>
                    </tr>
                    <tr>
                        <td>\(Magic\)</td>
                        <td>\(0.0001\)</td>
                        <td>\(0.995\)</td>
                        <td>\(0.00010\)</td>
                        <td>\(0.01951\)</td>
                    </tr>
                    <tr>
                        <td>\(Muggle\)</td>
                        <td>\(0.9999\)</td>
                        <td>\(0.005\)</td>
                        <td>\(0.00500\)</td>
                        <td>\(0.98049\)</td>
                    </tr>
                    <tr>
                        <td>\(total\)</td>
                        <td></td>
                        <td>\(1\)</td>
                        <td>\(0.00510\)</td>
                        <td>\(1\)</td>
                    </tr>
                    </tbody>
                </table>
            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
            <aside class="notes" data-markdown>
                * This table is also saying there's a 98% chance you're a boring Muggle even if you receive a Hogwarts
                letter.
            </aside>
        </section>
        <section
                data-transition="fade-in">
            <div data-markdown>
                ## Base Rate Fallacy

                The probability that a person - who receives a Hogwarts acceptance letter - and actually has magical
                potential is surprisingly low, despite the test being 99.5% accurate.
            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
            <aside class="notes">
                <ul>
                    <li>Suppose the population was in fact ten thousand people. Then the expected number of people who
                        actually have magical potential is 1.
                    </li>
                    <li>Of the remaining 9,999 people, 0.5% of them will test positive. which will be 50 other people.
                    </li>
                    <li>Okay who can tell me what can be done that might raise our chances?</li>
                </ul>
            </aside>
        </section>
        <section
                data-transition="fade-in">
            <div>
                <h2>Sequential Updating</h2>
                <p><em>Wait for another letter!</em></p>

                <div class="fragment">

                    <table>
                        <thead style="font-size: 85%">
                        <tr>
                            <th>Hypothesis</th>
                            <th>Prior 2</th>
                            <th>Likelihood</th>
                            <th>Numerator 2</th>
                            <th>Posterior 2</th>
                        </tr>
                        </thead>
                        <tbody style="font-size: 85%">
                        <tr>
                            <td>\(Magic\)</td>
                            <td>\(0.01951\)</td>
                            <td>\(0.995\)</td>
                            <td>\(0.01242\)</td>
                            <td>\(0.79841\)</td>
                        </tr>
                        <tr>
                            <td>\(Muggle\)</td>
                            <td>\(0.98049\)</td>
                            <td>\(0.005\)</td>
                            <td>\(0.00490\)</td>
                            <td>\(0.20159\)</td>
                        </tr>
                        <tr>
                            <td>\(total\)</td>
                            <td></td>
                            <td>\(1\)</td>
                            <td>\(0.02432\)</td>
                            <td>\(1\)</td>
                        </tr>
                        </tbody>
                    </table>
                    <p><em>A second letter means there's a 79.8% probability you aren&rsquo;t a muggle.</em></p>
                </div>
            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
            <aside class="notes">
                <ul>
                    <li>Run the test again!</li>
                    <li>Remember in Bayesian statistics, probability is subjective, and you can change what you know as
                        more information comes in.
                    </li>
                </ul>
            </aside>
        </section>
        <section
                data-transition="fade-in">
            <div>
                <h2>Continuous Priors</h2>
                <p><em>Bayes&rsquo; Theorem with continuous notation</em></p>
                <span class="math-formula">$$p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)}$$</span>

                <div class="fragment">
                    <p><em>Solving discrete pmf requires sums;<br/>solving continuous pdf requires integrals</em></p>
                    <span class="math-formula">$${f(\theta|x)d\theta = \frac{p(x|\theta)f(\theta)d\theta}{\int_a^b p(x|\theta)f(\theta)d\theta}}$$</span>
                </div>

            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
            <aside class="notes" data-markdown>
                * Anything above .300 is very good, anything below .215 is terrible
                * A player scoring a hit from 2 plate appearance has .500 average
                * A player striking out in first 1 appearances has a .000 average
            </aside>
        </section>
        <section
                data-transition="fade-in">
            <div>
                <h2>Flat Priors</h2>
                <p><em>Sometimes we don't know the prior distribution.</em></p>

                <ul class="fragment">
                    <li>Unlike Frequentist Statistics, Michael Bay fans want to model the uncertainty about the
                        probability of events
                    </li>
                    <li>If we know nothing, we assume a uniform distribution of prior knowledge \(f(\theta)d\theta =
                        1d\theta\)
                    </li>
                    <li>Example: we have a mystery coin. The probability \(\theta\) of landing heads can be anything.
                    </li>
                </ul>

            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
            <aside class="notes" data-markdown>
                * Here is one of the weaknesses of Bayesian statistics.
            </aside>
        </section>
        <section
                data-transition="fade-in">
            <div>
                <h2>Coin flips with calculus</h2>
                <ul>
                    <li>If the probability of heads can be anything, what is the probability it lands heads?<br/>
                        <span>\({P(\theta>.5)=\int_{.5}^1 f(\theta)d(\theta) = \int_{.5}^1 1 \cdot d(\theta) = \theta \Big|_{.5}^1= \frac{1}{2}}\)</span>
                    </li>
                    <li class="fragment">If it lands heads, what is the probability it favors heads?<br/>
                        <span style="font-size: 0.9em;">\({P(\theta>.5|x=1)=\int_{.5}^1 f(\theta|x=1)d(\theta) = \int_{.5}^1 2\theta d\theta = \theta^2 \Big|_{.5}^1= \frac{3}{4}}\)</span>
                    </li>
                    <li class="fragment">Surely, there&rsquo;s a better way to do this.</li>
                </ul>

            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
            <aside class="notes" data-markdown>
            </aside>
        </section>
        <section
                data-transition="fade-in">
            <div>
                <h2>Hey, coin tosses are binomial!</h2>

                <ul>
                    <li>Flipping the mystery coin 12 times gets 8 heads</li>
                    <li>What&rsquo;s the probability of \(k\) heads from \(n\) tosses?</li>
                </ul>

                <table class="fragment">
                    <tr style="font-size: 0.75em;">
                        <th>hypothesis</th>
                        <th>prior</th>
                        <th>likelihood</th>
                        <th>Bayes Numerator</th>
                        <th>Posterior</th>
                    </tr>
                    <tr style="font-size: 0.6em;">
                        <td>\(\theta\)</td>
                        <td>\(1 \cdot d\theta\)</td>
                        <td>\(\begin{pmatrix}12 \\8 \end{pmatrix}\theta^{8}(1-\theta)^{4}\)</td>
                        <td>\(\begin{pmatrix}12 \\8 \end{pmatrix}\theta^{8}(1-\theta)^{4}d\theta\)</td>
                        <td>\(c_2\theta^{8}(1-\theta)^{4}d\theta\)</td>
                    </tr>
                </table>

                <p class="fragment">We don't know what \(c_2\) is, but we know it&rsquo;s obtained from doing an
                    integral of a binomial function</p>

            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
            <aside class="notes" data-markdown>
                * Even for unknown parameters, it's going to be binomial
            </aside>
        </section>
        <section
                data-transition="fade-in">
            <div>
                <h2>Insert Baseball Analogy</h2>

                <ul>
                    <li>A player&rsquo;s batting average is the proportion of hits to total number of plate
                        appearances.
                    </li>
                    <li>At the beginning a season the batting average is meaningless if you ignore prior knowledge</li>
                    <li>What we want is a way of updating a player&rsquo;s batting average over time while not
                        fluctuated wildly
                    </li>
                </ul>

            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
            <aside class="notes" data-markdown>
                * Anything above .300 is very good, anything below .215 is terrible
                * A player scoring a hit from 2 plate appearance has .500 average
                * A player striking out in first 1 appearances has a .000 average
            </aside>
        </section>
        <section
                data-transition="fade-in">
            <div>
                <h2>The Beta Distribution</h2>
                <p><em>\(beta(\alpha,\beta)\) 2-parameter distribution with range \([0,1]\)</em></p>
                <span class="math formula">$${f(\theta) = c\theta^{\alpha-1}(1-\theta)^{\beta-1} = \frac{(\alpha+\beta-1)!}{(\alpha-1)!(\beta-1)!}\theta^{\alpha-1}(1-\theta)^{\beta-1}}$$</span>
                <ul class="fragment">
                    <li>\(\alpha\) and \(\beta\) are called <strong>hyperparameters</strong>, they are different than
                        the hypothesis parameter \(\theta\)
                    </li>
                    <li>That fraction part \(c\) is called a normalizing constant.</li>
                </ul>
            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
            <aside class="notes" data-markdown>
                * Suppose last season a player had 80 hits and 220 outs for a total of 300 at-bats
                * The batting average of this player is a beta distribution \\(beta(80,220)\\)
                * the pdf for a beta distribution should look familiar. Think of it as a meta-binomial distribution. It
                is distribution for the probabilities for a binomial distribution, which is itself a distribution for a
                bunch of bernoulli trials
            </aside>
        </section>
        <section
                data-transition="fade-in">
            <div>
                <h2>Conjugate Priors</h2>
                <p><em>If the posterior is the same probability distribution family as the the prior, then they are
                    conjugate distributions</em></p>
                <ul>
                    <li>If the likelihood function is binomial, and the prior distribution is beta, then the posterior is also beta.</li>
                    <li>Fewer integrals, hooray!</li>
                </ul>
            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
        </section>
        <section
                data-transition="fade-in">
            <div>
                <h2>Beta Constants</h2>

                <table style="font-size: 0.65em;margin: 1em 0;">
                    <tr>
                        <th>hypothesis</th>
                        <th>prior</th>
                        <th>likelihood</th>
                        <th>Bayes Numerator</th>
                        <th>Posterior</th>
                    </tr>
                    <tr>
                        <td>\(\theta\)</td>
                        <td>\(x\)</td>
                        <td>\(Beta(a, b)\)</td>
                        <td>\(Bin(n + m,\theta)\)</td>
                        <td>\(Beta(a + n,b+m)\)</td>
                    </tr>
                    <tr>
                        <td>\(\theta\)</td>
                        <td>\(x\)</td>
                        <td>\(c_1\theta^{a-1}(1-\theta)^{b-1}d\theta\)</td>
                        <td>\(c_2\theta^{x}(1-\theta)^{N-x}\)</td>
                        <td>\(c_3\theta^{a+x-1}(1-\theta)^{b+N-x-1}\)</td>
                    </tr>
                </table>

                <ul>
                    <li>Prior
                        <span>\(c_1 = \frac{(a+b-1)!}{(a-1)!(b-1)!}\)</span></li>
                    <li>Likelihood
                        <span>\({c_2 = \begin{pmatrix}N \\x \end{pmatrix} = \frac{N!}{x!(N-x)!}}\)</span></li>
                    <li>Posterior Constant
                        <span>\(c_3 = \frac{(a+b+N-1)!}{(a+x-1)!(b+N-x-1)!}\)</span></li>
                </ul>
            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
        </section>
        <section
                data-transition="fade-in">
            <div>
                <h2>Comparing 2 Betas</h2>
                <p><em>Treatment \((T)\) and Control \((C)\) are 2 beta distributions.</em></p>
                <span>$$p_C \sim Beta(\alpha_C,\beta_C), p_T \sim Beta(\alpha_T,\beta_T)$$</span>
                <div class="fragment">
                    <p><em>Then the probability that \(p_T\) is better than \(p_C\) is:</em></p>
                    <span style="font-size: 0.8em;">$${\rm Pr}(p_T > p_C) = \int_0^1 \int_{p_C}^1 \frac{p_C^{\alpha_C-1}(1-p_C)^{\beta_C-1}}{Beta(\alpha_C, \beta_C)} \frac{{p_T}^{\alpha_T-1}(1-p_T)^{\beta_T-1}}{Beta(\alpha_T, \beta_T)} dp_T dp_C$$</span>
                    <!--<span>$${P(p_T > p_C) = \sum_{i=0}^{\alpha_T-1} \frac{Beta(\alpha_C+i,\beta_T+\beta_C)}{(\beta_T+i)Beta(1+i,\beta_T)Beta(\alpha_C,\beta_C)}}$$</span>-->
                    <p><em>They call it <strong>&ldquo;Asymptotic Expansion.&rdquo;</strong> Uh huh, cool story.</em></p>
                </div>
            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
            <aside class="notes" data-markdown>
                * To get total probability that the true probability of the Treatment is greater than the true probability of the Control, we inegrate the join distribution over all the values for which the probability of Treatment is greater than the probability of Control.
                * One of the neat advantages of asymptotic expansion, or so I'm told, is that it allows us to do two things
                * know when to stop a test when there's obviously a clear winner
                * calculate the loss function, which is a measure of uncertainty from stopping a test prematurely and getting the wrong answer
                * Some day, I'd love to understand how to calculate this, but we're a room full of software engineers, which means we're lazy.
            </aside>
        </section>
        <section
                data-transition="fade-in">
            <div>
                <h2>Monte Carlo Simulations</h2>
                <ul>
                    <li>Use repeated random numbers to solve math problems where the analytical solution isn&rsquo;t
                        known, or intractable.
                    </li>
                    <li class="fragment">Often used to solve stupid integrals</li>
                    <li class="fragment">In other words, we can solve very complicated Bayesian posteriors with code!
                    </li>
                    <li class="fragment">Have to be careful because they can get computationally intensive</li>
                </ul>
            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
            <aside class="notes" data-markdown>
                * show how to calculate pi
                * generate a million [x,y] tuples where x and y are between 0 and 1. then graph it
                * Pythagorean rule \\(A^2 + B^2 = C^2\\)
            </aside>
        </section>
        <section
                data-transition="fade-in">
            <div>
                <h2>Monte Carlo pseudo code</h2>
                <pre class="fat-code"><code data-trim data-noescape>
                    // Observing the current app for 200 users (prior)
                    // Prior distribution is Beta(40,160)
                    // Initiate an A/B Test
                    // Control group got 35 clicks from 200 users (165 misses)
                    // Treatment group got 54 clicks from 200 users (146 misses)
                    // Simulate 100,000 Monte Carlo runs

                    simulations = 100000;
                    controlSimulation = beta(40 + 35, 160 + 165)(random(0,1)) × simulations;
                    treatmentSimulation = beta(40 + 54, 160 + 146)(random(0,1)) × simulations;

                    treatmentWins = total(array(controlSimulation) - array(treatmentSimulation));
                    treatmentWinProbability = treatmentWins / simulationsCount;
                </code></pre>
            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
        </section>
        <section
                data-transition="fade-in">
            <div>
                <h2>Monte Carlo A/B</h2>
                <pre class="fat-code"><code data-trim data-noescape>
                    from __future__ import division
                    from scipy.stats import beta
                    import numpy as np

                    sims = 100000 # number of simulations
                    p_alpha, p_n = 40, 200 # prior
                    p_beta = p_n - p_alpha
                    c_alpha, c_n = 35, 200 # control
                    c_beta = c_n - c_alpha
                    t_alpha, t_n = 54, 200 #treatment
                    t_beta = t_n - t_alpha

                    control = np.array(beta.rvs(p_alpha + c_alpha, p_beta + c_beta, size=sims))
                    treatment = np.array(beta.rvs(p_alpha + t_alpha, p_beta + t_beta, size=sims))

                    mc_treatment_wins = sum((treatment - control) > 0)
                    treatment_better_prob = mc_treatment_wins/simulations
                    print(treatment_better_prob)
                </code></pre>
            </div>
            <div class="slide-footer-left">
                <p class="footer-text">3. Bayesian Statistics</p>
            </div>
        </section>
        <section
                data-background-video="custom/media/baby-penguins.webm"
                data-background-video-loop="true"
                data-transition="fade-in">
        </section>
        <section
                data-transition="fade-in">
            <div>
                <h1><span style="text-decoration: line-through;">4. Multivariate Testing</span></h1>
                <p><em>Sorry, this a whole talk onto itself.</em></p>

                <ul>
                    <li>Full vs. Fractional Factorial</li>
                    <li>Chi-Squared Tests</li>
                    <li>Analysis of Covariance</li>
                    <li>Multilevel Modeling</li>
                    <li>Markov Chain Monte Carlo (MCMC)</li>
                </ul>
            </div>
        </section>
        <section
                data-transition="fade-in">
            <h2>The End</h2>
            <img src="../media/red-panda.jpg" style="height: 45vh;"/>
            <p>Is it lunch yet? Who's hungry?</p>
        </section>
    </div>
</div>

<script src="../../../lib/js/head.min.js"></script>
<script src="../../../js/reveal.js"></script>

<script>
    // More info about config & dependencies:
    // - https://github.com/hakimel/reveal.js#configuration
    // - https://github.com/hakimel/reveal.js#dependencies
    Reveal.initialize({
        slideNumber: 'c/t',
        showSlideNumber: 'speaker',
        controls: false,
        progress: false,
        history: true,
        autoPlayMedia: null,
//        center: true,
        width: 1080,
        height: 720,
//        margin: 0,
        math: {
            // mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
            config: 'TeX-AMS_HTML-full'
        },
        dependencies: [
            {src: '../../../plugin/markdown/marked.js'},
            {src: '../../../plugin/markdown/markdown.js'},
            {src: '../../../plugin/math/math.js', async: true},
            {src: '../../../plugin/notes/notes.js', async: true},
            {
                src: '../../../plugin/highlight/highlight.js', async: true, callback: function () {
                hljs.initHighlightingOnLoad();
            }
            }
        ]
    });
</script>
</body>
</html>
